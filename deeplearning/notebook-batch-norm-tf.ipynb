{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzx/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step1 load MNITST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the DNN network\n",
    "| Layer | Structure |  BN | Activation |\n",
    "|:-----:|:---------:|:---:|:----------:|\n",
    "| Input |[None, 784]| --- |    ---     |\n",
    "| FC1   |[None, 512]| 512 |    ReLU    |\n",
    "| FC2   |[None, 256]| 256 |    ReLU    |\n",
    "| FC3   |[None, 128]| 128 |    ReLU    |\n",
    "| Output|[None, 10] | --- |    tanh    |  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10]) # one-hot 10-dimensional vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layers\n",
    "# Refer to tf.nn.moments\n",
    "# FC1\n",
    "l_FC1 = 512\n",
    "W_FC1 = tf.Variable(tf.truncated_normal(shape=[784,l_FC1], stddev=0.1))\n",
    "b_FC1 = tf.Variable(tf.constant(0.1,shape=[l_FC1]))\n",
    "\n",
    "# batch_norm_l1\n",
    "x_FC1 = tf.matmul(x, W_FC1)+b_FC1\n",
    "axis = list(range(len(x_FC1.get_shape()) - 1))\n",
    "mean_FC1,var_FC1 = tf.nn.moments(x_FC1, axis)\n",
    "gamma_FC1 = tf.Variable(tf.constant(0.1, shape=mean_FC1.get_shape()))\n",
    "beta_FC1 = tf.Variable(tf.constant(0.1, shape=mean_FC1.get_shape()))\n",
    "y_bn1 = tf.nn.batch_normalization(\n",
    "    x = x_FC1,\n",
    "    mean = mean_FC1,\n",
    "    variance = var_FC1,\n",
    "    offset = beta_FC1,\n",
    "    scale = gamma_FC1,\n",
    "    variance_epsilon = 1e-5,\n",
    "    name= 'BN_FC1')\n",
    "y_FC1 = tf.nn.relu(y_bn1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FC2\n",
    "l_FC2 = 256\n",
    "W_FC2 = tf.Variable(tf.truncated_normal(shape=[l_FC1,l_FC2], stddev=0.1))\n",
    "b_FC2 = tf.Variable(tf.constant(0.1,shape=[l_FC2]))\n",
    "\n",
    "# batch_norm_l2\n",
    "x_FC2 = tf.matmul(y_FC1, W_FC2)+b_FC2\n",
    "axis = list(range(len(x_FC2.get_shape()) - 1))\n",
    "mean_FC2,var_FC2 = tf.nn.moments(x_FC2, axis)\n",
    "gamma_FC2 = tf.Variable(tf.constant(0.1, shape=mean_FC2.get_shape()))\n",
    "beta_FC2 = tf.Variable(tf.constant(0.1, shape=mean_FC2.get_shape()))\n",
    "y_bn2 = tf.nn.batch_normalization(\n",
    "    x = x_FC2,\n",
    "    mean = mean_FC2,\n",
    "    variance = var_FC2,\n",
    "    offset = beta_FC2,\n",
    "    scale = gamma_FC2,\n",
    "    variance_epsilon = 1e-5,\n",
    "    name= 'BN_FC2')\n",
    "y_FC2 = tf.nn.relu(y_bn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FC3\n",
    "l_FC3 = 128\n",
    "W_FC3 = tf.Variable(tf.truncated_normal(shape=[l_FC2,l_FC3], stddev=0.1))\n",
    "b_FC3 = tf.Variable(tf.constant(0.1,shape=[l_FC3]))\n",
    "\n",
    "# batch_norm_l3\n",
    "x_FC3 = tf.matmul(y_FC2, W_FC3)+b_FC3\n",
    "axis = list(range(len(x_FC3.get_shape()) - 1))\n",
    "mean_FC3,var_FC3 = tf.nn.moments(x_FC3, axis)\n",
    "gamma_FC3 = tf.Variable(tf.constant(0.1, shape=mean_FC3.get_shape()))\n",
    "beta_FC3 = tf.Variable(tf.constant(0.1, shape=mean_FC3.get_shape()))\n",
    "y_bn3 = tf.nn.batch_normalization(\n",
    "    x = x_FC3,\n",
    "    mean = mean_FC3,\n",
    "    variance = var_FC3,\n",
    "    offset = beta_FC3,\n",
    "    scale = gamma_FC3,\n",
    "    variance_epsilon = 1e-5,\n",
    "    name= 'BN_FC3')\n",
    "y_FC3 = tf.nn.relu(y_bn3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax layer\n",
    "l_soft = 10\n",
    "W_s = tf.Variable(tf.truncated_normal(shape=[l_FC3,l_soft], stddev=0.1))\n",
    "b_s = tf.Variable(tf.constant(0.1,shape=[l_soft]))\n",
    "\n",
    "x_s = tf.matmul(y_FC3, W_s)+b_s\n",
    "y = tf.nn.tanh(x_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function: cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss function\n",
    "# Note: labels and logits should be clearly assigned\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying the steepest gradient descent, with a step of 0.5, to descend the cross entropy\n",
    "# Tensorflow then add new operations to the computation graph, including ones to compute gradients, \n",
    "# compute parameters update steps, and apply update steps to the parameters.\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a session\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-01-24: 10:39:49]: Epochs 10\n",
      "[2018-01-24: 10:39:49]: Epochs 20\n",
      "[2018-01-24: 10:39:49]: Epochs 30\n",
      "[2018-01-24: 10:39:49]: Epochs 40\n",
      "[2018-01-24: 10:39:49]: Epochs 50\n",
      "[2018-01-24: 10:39:49]: Epochs 60\n",
      "[2018-01-24: 10:39:49]: Epochs 70\n",
      "[2018-01-24: 10:39:49]: Epochs 80\n",
      "[2018-01-24: 10:39:49]: Epochs 90\n",
      "[2018-01-24: 10:39:49]: Epochs 100\n"
     ]
    }
   ],
   "source": [
    "# Training the model by repeatedly running train_step\n",
    "import time \n",
    "epochs = 100\n",
    "batchsize= 100\n",
    "for i in range(epochs):\n",
    "    batch = mnist.train.next_batch(batchsize)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "    if (i+1) % 10 == 0:\n",
    "        timestamp = time.strftime('%Y-%m-%d: %H:%M:%S', time.localtime(time.time()))\n",
    "        print(\"[%s]: Epochs %d\" % (timestamp, i+1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
