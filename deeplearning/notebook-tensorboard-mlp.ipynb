{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A demo for Graph visualization with TensorBoard\n",
    "# https://www.tensorflow.org/get_started/summaries_and_tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mzx/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step1 load MNITST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(x, l_in, l_out, l_name, \n",
    "             act_fn, BN_flag=True):\n",
    "    with tf.name_scope(l_name):\n",
    "        with tf.name_scope('weights'):\n",
    "            W = weight_variable(shape=[l_in, l_out])\n",
    "            variable_summaries(W)\n",
    "        with tf.name_scope('biases'):\n",
    "            b = bias_variable(shape=[l_out])\n",
    "            variable_summaries(b)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            x_h = tf.matmul(x, W) + b\n",
    "            tf.summary.histogram('x_h', x_h)\n",
    "        if BN_flag:\n",
    "            with tf.name_scope('BatchNorm'):  \n",
    "                axis = list(range(len(x.get_shape()) - 1))\n",
    "                mean,var = tf.nn.moments(x_h, axis)\n",
    "                with tf.name_scope('gamma'):\n",
    "                    gamma = tf.Variable(\n",
    "                        tf.constant(0.1, shape=mean.get_shape()))\n",
    "                    variable_summaries(gamma)\n",
    "                with tf.name_scope('beta'):\n",
    "                    beta = tf.Variable(\n",
    "                        tf.constant(0.1, shape=mean.get_shape()))\n",
    "                    variable_summaries(beta)\n",
    "                y = tf.nn.batch_normalization(\n",
    "                    x = x_h,\n",
    "                    mean = mean,\n",
    "                    variance = var,\n",
    "                    offset = beta,\n",
    "                    scale = gamma,\n",
    "                    variance_epsilon = 1e-5,\n",
    "                    name= 'BN')\n",
    "                tf.summary.histogram('y', y)\n",
    "            with tf.name_scope('activation'):\n",
    "                y_act = act_fn(y)\n",
    "                tf.summary.histogram('activation', y_act)\n",
    "        else:\n",
    "            with tf.name_scope('activation'):\n",
    "                y_act = act_fn(x_h)\n",
    "                tf.summary.histogram('activation', y_act)\n",
    "   \n",
    "    return y_act  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dropout_layer(layer, keep_prob):\n",
    "    with tf.name_scope('dropout'):\n",
    "        y_dropout = tf.nn.dropout(layer, keep_prob)\n",
    "    return y_dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a softmax regression model\n",
    "# Placeholders\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10]) # one-hot 10-dimensional vector\n",
    "with tf.name_scope('dropout'):\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    tf.summary.scalar('dropout_keep_probability', keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layers\n",
    "# Refer to tf.nn.moments\n",
    "# FC1\n",
    "l_FC1 = 512\n",
    "y_FC1 = nn_layer(x=x, l_in = 784, l_out=512, \n",
    "                 l_name='FC1',\n",
    "                 act_fn=tf.nn.relu, BN_flag=False)\n",
    "y_FC1_drop = dropout_layer(y_FC1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FC2\n",
    "l_FC2 = 256\n",
    "y_FC2 = nn_layer(x=y_FC1_drop, l_in = l_FC1, \n",
    "                 l_out=l_FC2, l_name='FC2',\n",
    "                 act_fn=tf.nn.relu, BN_flag=False)\n",
    "y_FC2_drop = dropout_layer(y_FC2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FC3\n",
    "l_FC3 = 128\n",
    "y_FC3 = nn_layer(x=y_FC2_drop, l_in = l_FC2, \n",
    "                 l_out=l_FC3, l_name='FC3',\n",
    "                 act_fn=tf.nn.relu, BN_flag=False)\n",
    "y_FC3_drop = dropout_layer(y_FC3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax\n",
    "l_s = 10\n",
    "y = nn_layer(x=y_FC3_drop, l_in = l_FC3, \n",
    "             l_out=l_s, l_name='softmax',\n",
    "             act_fn=tf.nn.tanh, BN_flag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss function\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y))\n",
    "tf.summary.scalar('loss', cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learning rate\n",
    "with tf.name_scope('train'):\n",
    "    with tf.name_scope('learning_rate'):\n",
    "        init_lr = tf.placeholder(tf.float32, name='LR')\n",
    "        global_step = tf.placeholder(tf.float32, name=\"global_step\")\n",
    "        decay_step = tf.placeholder(tf.float32, name=\"decay_step\")\n",
    "        decay_rate = tf.placeholder(tf.float32, name=\"decay_rate\")\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            learning_rate = init_lr ,\n",
    "            global_step = global_step,\n",
    "            decay_steps = decay_step,\n",
    "            decay_rate = decay_rate,\n",
    "            staircase=False,\n",
    "            name=None)\n",
    "    train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the variables\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the summaries and write to logdir\n",
    "logdir = './log'\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(logdir + '/train',\n",
    "                                      sess.graph)\n",
    "test_writer = tf.summary.FileWriter(logdir + '/test')\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_dict(train,batchsize=100,drop=0.5,lr_dict=None):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if train:\n",
    "        xs, ys = mnist.train.next_batch(batchsize)\n",
    "        f_dict = {x: xs, y_: ys, keep_prob:drop}\n",
    "        f_dict.update(lr_dict)\n",
    "    else:\n",
    "        xs, ys = mnist.test.images, mnist.test.labels\n",
    "        f_dict = {x: xs, y_: ys, keep_prob:1.0}\n",
    "      \n",
    "    return f_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-01-24: 18:24:49]: accuracy at step 0: 0.110999994\n",
      "[2018-01-24: 18:24:51]: accuracy at step 100: 0.63479996\n",
      "[2018-01-24: 18:24:53]: accuracy at step 200: 0.794\n",
      "[2018-01-24: 18:24:55]: accuracy at step 300: 0.8346001\n",
      "[2018-01-24: 18:24:57]: accuracy at step 400: 0.89100015\n",
      "[2018-01-24: 18:24:59]: accuracy at step 500: 0.90560013\n",
      "[2018-01-24: 18:25:02]: accuracy at step 600: 0.9139001\n",
      "[2018-01-24: 18:25:04]: accuracy at step 700: 0.92040014\n",
      "[2018-01-24: 18:25:06]: accuracy at step 800: 0.9240001\n",
      "[2018-01-24: 18:25:08]: accuracy at step 900: 0.93120015\n"
     ]
    }
   ],
   "source": [
    "# Training the model by repeatedly running train_step\n",
    "import time \n",
    "epochs = 1000\n",
    "batchsize= 100\n",
    "\n",
    "lr_init = 0.5\n",
    "d_rate = 0.9\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i % 100 == 0:\n",
    "        timestamp = time.strftime('%Y-%m-%d: %H:%M:%S', time.localtime(time.time()))\n",
    "        summary, acc = sess.run(\n",
    "            [merged, accuracy],feed_dict=feed_dict(False))\n",
    "        test_writer.add_summary(summary, i)\n",
    "        print('[%s]: accuracy at step %s: %s' % (timestamp, i, acc))\n",
    "    else:\n",
    "        lr_dict = {init_lr: lr_init, global_step:i,\n",
    "                   decay_step: i, decay_step: batchsize,\n",
    "                   decay_rate: d_rate}\n",
    "        summary, _ = sess.run(\n",
    "            [merged, train_step], \n",
    "            feed_dict=feed_dict(True,lr_dict=lr_dict))\n",
    "        train_writer.add_summary(summary, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'FC1/weights/Variable:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'FC1/biases/Variable:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'FC2/weights/Variable:0' shape=(512, 256) dtype=float32_ref>,\n",
       " <tf.Variable 'FC2/biases/Variable:0' shape=(256,) dtype=float32_ref>,\n",
       " <tf.Variable 'FC3/weights/Variable:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'FC3/biases/Variable:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax/weights/Variable:0' shape=(128, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax/biases/Variable:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.global_variables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
